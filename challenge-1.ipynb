{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "Welcome to Week 2’s challenge notebook! We’ll do two main things:\n",
    "\n",
    "1. **Answer a few ML refresher questions** (multiple-choice style) from last week’s live knowledge check quiz. \n",
    "2. **Train a classifier** on the **Breast Cancer** dataset, achieving a good test accuracy. You can choose your classifier model from sklearn.\n",
    "\n",
    "When you fill in this notebook and push it to GitHub, an external Pytest script will check:\n",
    "- Whether your MCQ answers are correct  \n",
    "- If your classifier’s test accuracy is above a certain threshold using a hidden dataset.\n",
    "\n",
    "Section specific details and instructions are provided where necessary.\n",
    "\n",
    "**Good luck!**\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: MCQ Functions\n",
    "\n",
    "Below are **3 refresher questions** from last week’s ML basics. Each function has a TODO where you **return** the option choice from the `options` dictionary. All you need to do is to fill in the option number in the designated area.\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "options = {  \n",
    "    1: 'option a',  \n",
    "    2: 'option b',  \n",
    "    3: 'option c'  \n",
    "}  \n",
    "  \n",
    "return options[1]  \n",
    "```\n",
    "\n",
    "**Notes**:\n",
    "- *If you fail to return the exact expected answer, autograding will fail the testcase.*  \n",
    "- Each question states the options within the declared `options` dictionary or question text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. \"Which type of ML uses labeled data?\"\n",
    "\n",
    "**Options**:  \n",
    "1. supervised\n",
    "2. unsupervised\n",
    "3. reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Which type of ML uses labeled data?\n",
      "supervised learning\n"
     ]
    }
   ],
   "source": [
    "def answer_q1():\n",
    "    \"\"\"\n",
    "    Q1: Which type of ML uses labeled data?\n",
    "    \"\"\"\n",
    "    \n",
    "    options = {\n",
    "        1: \"supervised\",\n",
    "        2: \"unsupervised\",\n",
    "        3: \"reinforcement\"\n",
    "    }\n",
    "\n",
    "    # TODO: return the correct option number\n",
    "    return options[1]\n",
    "\n",
    "print(f'Q1: Which type of ML uses labeled data?\\n'\n",
    "      f'{answer_q1()} learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q2. \"Which approach is typically used to *quantify* the difference between predicted outputs and actual labels?\"\n",
    "\n",
    "**Options**: \n",
    "1. accuracy metric\n",
    "2. loss function\n",
    "3. gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2: Which approach is typically used to measure differences between predictions and actual labels?\n",
      "loss function\n"
     ]
    }
   ],
   "source": [
    "def answer_q2():\n",
    "    \"\"\"\n",
    "    Q2: Which approach is typically used to quantify the difference\n",
    "        between predicted outputs and actual labels?\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        1: \"accuracy metric\",\n",
    "        2: \"loss function\",\n",
    "        3: \"gradient descent\"\n",
    "    }\n",
    "    \n",
    "    # TODO: return the correct option number\n",
    "    return options[2]\n",
    "\n",
    "print(f'Q2: Which approach is typically used to measure differences between predictions and actual labels?\\n'\n",
    "      f'{answer_q2()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. \"What best distinguishes classification from regression?\"\n",
    "\n",
    "**Options**:\n",
    "1. Regression => continuous, Classification => discrete\n",
    "2. Both produce continuous outputs\n",
    "3. Regression => discrete, Classification => continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: What best distinguishes classification from regression?\n",
      "Regression => continuous, Classification => discrete\n"
     ]
    }
   ],
   "source": [
    "def answer_q3():\n",
    "    \"\"\"\n",
    "    Q3: What best distinguishes classification from regression?\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        1: \"Regression => continuous, Classification => discrete\",\n",
    "        2: \"Both produce continuous outputs\",\n",
    "        3: \"Regression => discrete, Classification => continuous\"\n",
    "    }\n",
    "    \n",
    "    # TODO: return the correct option number\n",
    "    return options[1]\n",
    "\n",
    "print(f'Q3: What best distinguishes classification from regression?\\n'\n",
    "      f'{answer_q3()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Code challenge\n",
    "\n",
    "To pass this section of the weekly challenge, uncomment/fill in code where necessary (marked with a 'TODO:' comment).\n",
    "\n",
    "### 1. House keeping and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: Import your classifier model of choice below.\n",
    "# Example:\n",
    "# from sklearn.linear_model import Perceptron \n",
    "# The above is just an example, feel free to use any classifier like DecisionTreeClassifier, LogisticRegression, etc. based on your results and experimentation.\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Load and Inspect the Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_cancer_data():\n",
    "    \"\"\"\n",
    "    Loads breast cancer dataset from sklearn.\n",
    "    Returns X (np.array), y (np.array), and feature_names (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    cancer_dataframe = pd.read_csv('breast_cancer.csv') # loading the dataset from a CSV file into a pandas DataFrame\n",
    "    X = cancer_dataframe.drop('target', axis=1).values # Selecting all columns except the target column.\n",
    "    \n",
    "    # If you want to select specific features (maybe some features are more correlated to the target than others), you can do so like this:\n",
    "    # X = cancer[['mean radius', 'mean texture']].values\n",
    "\n",
    "    y = cancer_dataframe['target'].values # Selecting the target column\n",
    "\n",
    "    return X, y, cancer_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful visualisation examples for the dataset\n",
    "Run the below code cells for examples of data visualisations that can help you understand the data and potentially help with feature selection should you choose to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all, dataframe = load_cancer_data()\n",
    "\n",
    "# Displaying the first 5 rows\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe has:\n",
    "\n",
    "- data: shape (500, 30) i.e., 500 rows or data points and 30 columns or features.\n",
    "- target: shape (500,) with 0 or 1 (malignant vs. benign).\n",
    "\n",
    "The specification of each column and common metadata of the dataframe can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple correlation heatmap of a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subset_cols = list(dataframe.columns[:5])\n",
    "corr = dataframe[subset_cols + [\"target\"]].corr()\n",
    "# Example heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(corr, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Correlation (first 5 features + target)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot of two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scatter plot\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(data=dataframe, x=\"mean radius\", y=\"mean texture\", hue=\"target\")\n",
    "plt.title(\"Scatter plot of mean radius vs mean texture\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore and visualise the dataset to help you understand the data better. \n",
    "\n",
    "**Note:** Data exploration is not graded for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Train a Classifier\n",
    "\n",
    "We want a function `train_cancer_classifier()` that:\n",
    "\n",
    "1. Loads `(X, y)`\n",
    "2. Splits into train/test.\n",
    "3. Trains a simple **classifier model**.\n",
    "4. Returns trained `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cancer_classifier(test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    1. load data (X, y)\n",
    "    2. split (train/test)\n",
    "    3. train classifier model\n",
    "    4. return model\n",
    "    \"\"\"\n",
    "    # TODO: load X, y\n",
    "    # X, y, dataframe = ...\n",
    "    \n",
    "    # TODO: do train_test_split with test_size and random_state\n",
    "    # Use X and y to split into X_train, X_test, y_train, y_test\n",
    "    # X_train, X_test, y_train, y_test = ...\n",
    "    \n",
    "    # TODO: create the model\n",
    "    # Remember to import your model of choice from sklearn in the imports cell above\n",
    "    # Do not remove random_state so that your model is reproducible which is needed for fair grading\n",
    "    # example:\n",
    "    # model = Perceptron(max_iter=500, random_state=random_state)\n",
    "\n",
    "    # TODO: train the model\n",
    "    # model.fit(...)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predict Function\n",
    "\n",
    "Given a trained `model`, test data `X_test` and corresponding target labels `y_test`, we want to output class predictions (0 or 1).\n",
    "\n",
    "**Note**: This function will be used to test your model on a hidden dataset during autograding (completing this function is SUPER IMPORTANT!). So, your accuracy with the hidden dataset will determine the testcase result. The accuracy to beat is <INSERT TARGET ACCURACY SCORE>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cancer(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    model: a trained classifier model\n",
    "    X_test: shape (N, 30) if we keep all features\n",
    "    y_test: shape (N,) array of 0 or 1 of the test labels\n",
    "    returns (y_pred, accuracy) i.e., the predicted labels and the accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: predict on X_test\n",
    "    # y_pred = ...\n",
    "\n",
    "    # TODO: calculate accuracy\n",
    "    # accuracy = accuracy_score(...)\n",
    "    \n",
    "    return y_pred, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model\n",
    "\n",
    "Once finished, you can test your model by running the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_cancer_classifier(test_size=0.2, random_state=SEED)\n",
    "\n",
    "X, y, _ = load_cancer_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "y_pred, accuracy = predict_cancer(model, X_test, y_test)\n",
    "\n",
    "print(f\"Model's test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### End of Notebook\n",
    "\n",
    "Push your code. The autograder test script will do:\n",
    "1. Check all three MCQ functions' return values and pass/fail each test case\n",
    "2. `train_cancer_classifier()`\n",
    "3. Load the hidden dataset\n",
    "4. Evaluate your model on that hidden set\n",
    "5. Pass/fail if accuracy surpasses a threshold\n",
    "\n",
    "If you've made it this far successfully, congratulations on completing your first challenge assignment! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
